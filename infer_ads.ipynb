{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 06:01:29.274512: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 06:01:29.310911: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 06:01:29.310940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 06:01:29.311998: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 06:01:29.318683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 06:01:29.988574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_multi_tensor is not installed corrected\n",
      "fused_layer_norm is not installed corrected\n",
      "fused_softmax is not installed corrected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 06:01:32 | model/unimol.py | 114 | INFO | Uni-Mol(QSAR) | Loading pretrained weights from /vepfs/fs_users/ycjin/ads_predict_tools/weights/mol_pre_all_h_220816.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model import *\n",
    "from data import *\n",
    "from utils import *\n",
    "\n",
    "pth_save_path = './example/ads/pth/'\n",
    "\n",
    "mol2input = Mol2Input()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "fitting_model = FittingNet(output_dim=1)\n",
    "unimol_model = UniMolModel(output_dim=1, data_type='molecule', remove_hs=False)\n",
    "fitting_model.load_state_dict(torch.load(pth_save_path +'atomic_fit_nh.pth'))\n",
    "unimol_model.load_state_dict(torch.load(pth_save_path + 'atomic_model_nh.pth'))\n",
    "fitting_model.to(device)\n",
    "fitting_model.eval()\n",
    "unimol_model.to(device)\n",
    "unimol_model.eval()\n",
    "print('Load model successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单项预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0793],\n",
      "        [-0.0719],\n",
      "        [-0.0185],\n",
      "        [-0.0639],\n",
      "        [-0.0534],\n",
      "        [-0.0790],\n",
      "        [-0.0493],\n",
      "        [-0.0810],\n",
      "        [-0.0507],\n",
      "        [-0.0406],\n",
      "        [-0.0422],\n",
      "        [-0.0542],\n",
      "        [-0.0519],\n",
      "        [-0.0291],\n",
      "        [-0.0398],\n",
      "        [-0.0706],\n",
      "        [-0.0292],\n",
      "        [-0.0444],\n",
      "        [-0.0393],\n",
      "        [-0.0430],\n",
      "        [-0.0618],\n",
      "        [-0.0627],\n",
      "        [-0.0488],\n",
      "        [-0.0690],\n",
      "        [-0.0792],\n",
      "        [-0.0409],\n",
      "        [-0.0677],\n",
      "        [-0.0341],\n",
      "        [-0.0245],\n",
      "        [-0.0728],\n",
      "        [-0.0795],\n",
      "        [-0.0522],\n",
      "        [-0.0562],\n",
      "        [-0.0550],\n",
      "        [-0.0145],\n",
      "        [-0.0437],\n",
      "        [-0.0222],\n",
      "        [-0.0242],\n",
      "        [-0.0311],\n",
      "        [-0.0270],\n",
      "        [-0.0774],\n",
      "        [-0.0306],\n",
      "        [-0.0527],\n",
      "        [-0.0837],\n",
      "        [-0.0562],\n",
      "        [-0.0494],\n",
      "        [-0.0430],\n",
      "        [-0.0667],\n",
      "        [-0.0699],\n",
      "        [-0.0558],\n",
      "        [-0.0422],\n",
      "        [-0.0663],\n",
      "        [-0.0477],\n",
      "        [-0.0547],\n",
      "        [-0.0056],\n",
      "        [-0.0902],\n",
      "        [-0.0278],\n",
      "        [-0.0662],\n",
      "        [-0.0457],\n",
      "        [-0.0292],\n",
      "        [-0.0640],\n",
      "        [-0.0367],\n",
      "        [-0.0552],\n",
      "        [-0.0151],\n",
      "        [-0.0375],\n",
      "        [-0.0568],\n",
      "        [-0.0180],\n",
      "        [-0.0468],\n",
      "        [-0.0480],\n",
      "        [-0.0249],\n",
      "        [-0.0503],\n",
      "        [-0.0194],\n",
      "        [-0.0053],\n",
      "        [-0.0619],\n",
      "        [-0.0542],\n",
      "        [-0.0493],\n",
      "        [-0.0244],\n",
      "        [-0.0203],\n",
      "        [-0.0236],\n",
      "        [-0.0505],\n",
      "        [-0.0690],\n",
      "        [-0.0246],\n",
      "        [-0.0157],\n",
      "        [-0.0580],\n",
      "        [-0.0020],\n",
      "        [-0.0365],\n",
      "        [-0.0876],\n",
      "        [-0.0288],\n",
      "        [-0.0181],\n",
      "        [-0.0793],\n",
      "        [-0.0331],\n",
      "        [-0.0654],\n",
      "        [-0.0306],\n",
      "        [-0.0605],\n",
      "        [-0.0257],\n",
      "        [-0.0118],\n",
      "        [-0.0613],\n",
      "        [-0.0881],\n",
      "        [-0.0357],\n",
      "        [-0.0596],\n",
      "        [-0.0131],\n",
      "        [-0.0539],\n",
      "        [-0.0502],\n",
      "        [-0.0351],\n",
      "        [-0.0467],\n",
      "        [-0.0897],\n",
      "        [-0.0598],\n",
      "        [-0.0361],\n",
      "        [-0.0359],\n",
      "        [-0.0684],\n",
      "        [-0.0292],\n",
      "        [-0.0317],\n",
      "        [-0.0177],\n",
      "        [-0.0568],\n",
      "        [-0.0217],\n",
      "        [-0.0356],\n",
      "        [-0.0070],\n",
      "        [-0.0702],\n",
      "        [-0.0677],\n",
      "        [-0.0508],\n",
      "        [-0.0408],\n",
      "        [-0.0259],\n",
      "        [-0.0561],\n",
      "        [-0.0918],\n",
      "        [-0.0632],\n",
      "        [ 0.0113],\n",
      "        [-0.0628],\n",
      "        [-0.0285],\n",
      "        [-0.0228],\n",
      "        [-0.0521],\n",
      "        [-0.0624],\n",
      "        [-0.0577],\n",
      "        [-0.0197],\n",
      "        [-0.0614],\n",
      "        [-0.0447],\n",
      "        [-0.0567],\n",
      "        [-0.0396],\n",
      "        [ 0.0060],\n",
      "        [-0.0708],\n",
      "        [-0.0504],\n",
      "        [-0.0160],\n",
      "        [-0.0279],\n",
      "        [-0.0200],\n",
      "        [-0.0409],\n",
      "        [-0.0351],\n",
      "        [-0.0403],\n",
      "        [-0.0164],\n",
      "        [-0.0362],\n",
      "        [-0.0209],\n",
      "        [-0.0524],\n",
      "        [-0.0801],\n",
      "        [-0.0263],\n",
      "        [-0.0580],\n",
      "        [-0.0547],\n",
      "        [-0.0128],\n",
      "        [-0.0364],\n",
      "        [-0.0493],\n",
      "        [-0.0451],\n",
      "        [-0.0527],\n",
      "        [-0.0571],\n",
      "        [-0.0504],\n",
      "        [-0.0597],\n",
      "        [-0.0751],\n",
      "        [-0.0619],\n",
      "        [-0.0536],\n",
      "        [-0.0366],\n",
      "        [-0.0739],\n",
      "        [-0.0290],\n",
      "        [-0.0685],\n",
      "        [-0.0552],\n",
      "        [-0.0484],\n",
      "        [-0.0564],\n",
      "        [-0.0085],\n",
      "        [-0.0958],\n",
      "        [-0.0006],\n",
      "        [-0.0541],\n",
      "        [-0.0591],\n",
      "        [-0.0632],\n",
      "        [-0.0512],\n",
      "        [-0.0457],\n",
      "        [-0.0396],\n",
      "        [-0.0646],\n",
      "        [-0.0576],\n",
      "        [-0.0345],\n",
      "        [-0.0907],\n",
      "        [-0.0225],\n",
      "        [-0.0348],\n",
      "        [-0.0614],\n",
      "        [-0.0483],\n",
      "        [-0.0565],\n",
      "        [-0.0324],\n",
      "        [-0.0503],\n",
      "        [-0.0493],\n",
      "        [-0.0250],\n",
      "        [-0.0690],\n",
      "        [-0.0059],\n",
      "        [-0.0600],\n",
      "        [-0.0563],\n",
      "        [-0.0736],\n",
      "        [-0.0563],\n",
      "        [-0.0684],\n",
      "        [-0.0648],\n",
      "        [-0.0682],\n",
      "        [-0.0054],\n",
      "        [-0.0562],\n",
      "        [-0.0570],\n",
      "        [-0.0325],\n",
      "        [-0.0423],\n",
      "        [-0.0457],\n",
      "        [-0.0390],\n",
      "        [-0.0280],\n",
      "        [-0.0910],\n",
      "        [-0.0670],\n",
      "        [-0.0619],\n",
      "        [-0.0446],\n",
      "        [-0.0412],\n",
      "        [-0.0479],\n",
      "        [-0.0373],\n",
      "        [-0.0549],\n",
      "        [-0.0096],\n",
      "        [-0.0324],\n",
      "        [-0.0452],\n",
      "        [-0.0763],\n",
      "        [-0.0386],\n",
      "        [ 0.0060],\n",
      "        [-0.0549],\n",
      "        [-0.0007],\n",
      "        [-0.0715],\n",
      "        [-0.0504],\n",
      "        [-0.0438],\n",
      "        [-0.0234],\n",
      "        [-0.0308],\n",
      "        [ 0.0053],\n",
      "        [-0.0295],\n",
      "        [-0.0272],\n",
      "        [-0.0555],\n",
      "        [-0.0396],\n",
      "        [-0.0333],\n",
      "        [-0.0182],\n",
      "        [-0.0121],\n",
      "        [-0.0507],\n",
      "        [-0.0436],\n",
      "        [-0.0512],\n",
      "        [-0.0289],\n",
      "        [-0.0412],\n",
      "        [-0.0780],\n",
      "        [-0.0119],\n",
      "        [-0.0310],\n",
      "        [-0.0758],\n",
      "        [-0.0285],\n",
      "        [-0.0528],\n",
      "        [-0.0684],\n",
      "        [-0.0763],\n",
      "        [-0.0255],\n",
      "        [-0.0408],\n",
      "        [-0.0705],\n",
      "        [-0.0535],\n",
      "        [-0.0021],\n",
      "        [-0.0316],\n",
      "        [-0.0664],\n",
      "        [-0.0738],\n",
      "        [-0.0053],\n",
      "        [-0.0337],\n",
      "        [-0.0109],\n",
      "        [-0.0540],\n",
      "        [-0.0565],\n",
      "        [-0.0483],\n",
      "        [-0.0614],\n",
      "        [ 0.0055],\n",
      "        [-0.0387],\n",
      "        [-0.0471],\n",
      "        [-0.0560],\n",
      "        [-0.0503],\n",
      "        [-0.0376],\n",
      "        [-0.0709],\n",
      "        [-0.0583],\n",
      "        [-0.0246],\n",
      "        [-0.0658],\n",
      "        [-0.0580],\n",
      "        [-0.0750],\n",
      "        [-0.0489],\n",
      "        [-0.0413],\n",
      "        [-0.0285],\n",
      "        [-0.0429],\n",
      "        [-0.0521],\n",
      "        [-0.0520],\n",
      "        [-0.0371],\n",
      "        [-0.0364],\n",
      "        [-0.0514],\n",
      "        [-0.0247],\n",
      "        [-0.0308],\n",
      "        [-0.0599],\n",
      "        [-0.0533],\n",
      "        [-0.0084],\n",
      "        [-0.0428],\n",
      "        [ 0.0084],\n",
      "        [-0.0306],\n",
      "        [-0.0332],\n",
      "        [-0.0599],\n",
      "        [-0.0713],\n",
      "        [-0.0492],\n",
      "        [-0.0727],\n",
      "        [-0.0513],\n",
      "        [-0.0509],\n",
      "        [-0.0585],\n",
      "        [-0.0459],\n",
      "        [-0.0458],\n",
      "        [-0.0584],\n",
      "        [-0.0601],\n",
      "        [-0.0697],\n",
      "        [-0.0495],\n",
      "        [-0.0279],\n",
      "        [-0.0471],\n",
      "        [-0.0351],\n",
      "        [-0.0372],\n",
      "        [-0.0446],\n",
      "        [-0.0303],\n",
      "        [-0.0165],\n",
      "        [-0.0224],\n",
      "        [-0.0240],\n",
      "        [-0.0196],\n",
      "        [-0.0369],\n",
      "        [-0.0338],\n",
      "        [-0.0574],\n",
      "        [-0.0267],\n",
      "        [-0.0661],\n",
      "        [-0.0508],\n",
      "        [-0.0644],\n",
      "        [-0.0685],\n",
      "        [-0.0624],\n",
      "        [-0.0451],\n",
      "        [-0.0640],\n",
      "        [-0.0420],\n",
      "        [ 0.0005],\n",
      "        [-0.0470],\n",
      "        [-0.0408],\n",
      "        [-0.0782],\n",
      "        [-0.0836],\n",
      "        [-0.0727],\n",
      "        [-0.0252],\n",
      "        [-0.0608],\n",
      "        [-0.0753],\n",
      "        [-0.0356],\n",
      "        [-0.0577],\n",
      "        [-0.0665],\n",
      "        [ 0.0006],\n",
      "        [-0.0496],\n",
      "        [-0.0724],\n",
      "        [-0.0415],\n",
      "        [-0.0484],\n",
      "        [-0.0794],\n",
      "        [-0.0297],\n",
      "        [-0.0501],\n",
      "        [-0.0254],\n",
      "        [-0.0680],\n",
      "        [-0.0270],\n",
      "        [-0.0236],\n",
      "        [-0.0632],\n",
      "        [-0.0183],\n",
      "        [-0.0488],\n",
      "        [-0.0605],\n",
      "        [-0.0578],\n",
      "        [-0.0294],\n",
      "        [-0.0627],\n",
      "        [-0.0521],\n",
      "        [-0.0050],\n",
      "        [-0.0525],\n",
      "        [-0.0750],\n",
      "        [-0.0509],\n",
      "        [-0.0349],\n",
      "        [-0.0604],\n",
      "        [-0.0646],\n",
      "        [-0.0633],\n",
      "        [-0.0526],\n",
      "        [-0.0759],\n",
      "        [-0.0197],\n",
      "        [-0.0533],\n",
      "        [-0.0201],\n",
      "        [-0.0613],\n",
      "        [ 0.0090],\n",
      "        [-0.0555],\n",
      "        [ 0.0034],\n",
      "        [-0.0485],\n",
      "        [-0.0616],\n",
      "        [-0.0434],\n",
      "        [-0.0401],\n",
      "        [-0.0581],\n",
      "        [ 0.0076],\n",
      "        [-0.0440],\n",
      "        [-0.0299],\n",
      "        [-0.0359],\n",
      "        [-0.0155],\n",
      "        [-0.0500],\n",
      "        [-0.0422],\n",
      "        [-0.0580],\n",
      "        [-0.0515],\n",
      "        [-0.0494],\n",
      "        [-0.0225],\n",
      "        [-0.0270],\n",
      "        [-0.0393],\n",
      "        [-0.0122],\n",
      "        [-0.0609],\n",
      "        [-0.0518],\n",
      "        [-0.0535],\n",
      "        [-0.0520],\n",
      "        [-0.0453],\n",
      "        [-0.0851],\n",
      "        [-0.0534],\n",
      "        [-0.0762],\n",
      "        [-0.0338],\n",
      "        [-0.0638],\n",
      "        [-0.0587],\n",
      "        [-0.0612],\n",
      "        [-0.0274],\n",
      "        [-0.0780],\n",
      "        [-0.0669],\n",
      "        [-0.0212],\n",
      "        [-0.0477],\n",
      "        [-0.0663],\n",
      "        [-0.0461],\n",
      "        [-0.0173],\n",
      "        [-0.0351],\n",
      "        [-0.0460],\n",
      "        [-0.0314],\n",
      "        [-0.0299],\n",
      "        [-0.0229],\n",
      "        [-0.0530],\n",
      "        [-0.0701],\n",
      "        [-0.0480],\n",
      "        [-0.0885],\n",
      "        [-0.0526],\n",
      "        [-0.0207],\n",
      "        [-0.0849],\n",
      "        [-0.0376],\n",
      "        [-0.0218],\n",
      "        [-0.0565],\n",
      "        [-0.0500],\n",
      "        [-0.0409],\n",
      "        [-0.0278],\n",
      "        [-0.0611],\n",
      "        [-0.0394],\n",
      "        [-0.0720],\n",
      "        [-0.0456],\n",
      "        [-0.0271],\n",
      "        [-0.0300],\n",
      "        [-0.0275],\n",
      "        [-0.0136],\n",
      "        [-0.0316],\n",
      "        [-0.0766],\n",
      "        [-0.0470],\n",
      "        [-0.0324],\n",
      "        [-0.0626],\n",
      "        [-0.0555],\n",
      "        [-0.0541],\n",
      "        [-0.0539],\n",
      "        [ 0.0004],\n",
      "        [-0.0484],\n",
      "        [-0.0673],\n",
      "        [-0.0601],\n",
      "        [-0.0222],\n",
      "        [-0.0553],\n",
      "        [-0.0417],\n",
      "        [-0.0742],\n",
      "        [-0.0634],\n",
      "        [-0.0630],\n",
      "        [-0.0385],\n",
      "        [-0.0489],\n",
      "        [-0.0161],\n",
      "        [-0.0258],\n",
      "        [-0.0540],\n",
      "        [-0.0479],\n",
      "        [-0.0569],\n",
      "        [-0.0460],\n",
      "        [-0.0409],\n",
      "        [-0.0203],\n",
      "        [-0.0180],\n",
      "        [-0.0140],\n",
      "        [-0.0223],\n",
      "        [-0.0525],\n",
      "        [-0.0330],\n",
      "        [-0.0255],\n",
      "        [-0.0477],\n",
      "        [-0.0758],\n",
      "        [-0.0787],\n",
      "        [-0.0376],\n",
      "        [-0.0011],\n",
      "        [-0.0652],\n",
      "        [-0.0663],\n",
      "        [-0.0467],\n",
      "        [-0.0704],\n",
      "        [-0.0354],\n",
      "        [-0.0664],\n",
      "        [-0.0601],\n",
      "        [-0.0563],\n",
      "        [-0.0235],\n",
      "        [-0.0884],\n",
      "        [-0.0085],\n",
      "        [-0.0449],\n",
      "        [-0.0233],\n",
      "        [-0.0284],\n",
      "        [-0.0375],\n",
      "        [-0.0543],\n",
      "        [-0.0480],\n",
      "        [-0.0513],\n",
      "        [-0.0414],\n",
      "        [-0.0371],\n",
      "        [-0.0568],\n",
      "        [-0.0136],\n",
      "        [-0.0499],\n",
      "        [-0.0146],\n",
      "        [-0.0489],\n",
      "        [-0.0707]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor(-23.2335, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 512 is out of bounds for axis 0 with size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m view\u001b[38;5;241m.\u001b[39maddModel(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmd_n.xyz\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,\u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(atype[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 57\u001b[0m     view\u001b[38;5;241m.\u001b[39msetStyle({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m:i}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msphere\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m:setting[\u001b[38;5;28mtype\u001b[39m][\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mself_hex(\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m+\u001b[39mself_hex(value[i])\u001b[38;5;241m+\u001b[39mself_hex(value[i])}})\n\u001b[1;32m     58\u001b[0m view\u001b[38;5;241m.\u001b[39mzoomTo(animate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m view\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 512 is out of bounds for axis 0 with size 512"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "from ase.io import read,write\n",
    "import py3Dmol\n",
    "from ase.io import read\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# atom = read('temp.xyz')\n",
    "input_file = './example/pub25/data/xyz/400.xyz' # input molecule file\n",
    "atom = read(input_file)\n",
    "\n",
    "# 去除H原子\n",
    "sym = np.array(atom.get_chemical_symbols())\n",
    "coord = [torch.tensor(atom.get_positions())[sym != 'H']]\n",
    "atype = [np.array(atom.get_chemical_symbols())[sym != 'H']]\n",
    "atom = atom[sym != 'H']\n",
    "\n",
    "# 预测\n",
    "input_dict = mol2input.coord2unimol_inputs(coord,atype)\n",
    "for k in input_dict.keys(): input_dict[k] = input_dict[k].to(device)\n",
    "atomic_reprs = unimol_model(return_repr=True,**input_dict)['atomic_reprs']\n",
    "pred = []\n",
    "for repr in atomic_reprs:\n",
    "    atomic_p = fitting_model(repr)\n",
    "    p = torch.sum(fitting_model(repr))\n",
    "\n",
    "print(atomic_p)# 原子贡献\n",
    "print(p)# 总能量\n",
    "\n",
    "\n",
    "# 可视化\n",
    "value = np.array(atomic_p.detach().cpu()).reshape(-1)\n",
    "value = (value-np.min(value))/(np.max(value)-np.min(value)) * 200+50\n",
    "value = np.int16(value)\n",
    "\n",
    "setting = {\n",
    "    'H':[0.3],\n",
    "    'C':[0.3],\n",
    "    'O':[0.3],\n",
    "    'N':[0.3],\n",
    "    'S':[0.3],\n",
    "    'F':[0.3],\n",
    "    'Cl':[0.3],\n",
    "    'Br':[0.3],\n",
    "    'I':[0.3]\n",
    "}\n",
    "\n",
    "def self_hex(n):\n",
    "    return hex(n)[2:].zfill(2)\n",
    "\n",
    "write('md_n.xyz', atom)\n",
    "\n",
    "# 贡献图\n",
    "view = py3Dmol.view(width=300, height=300)\n",
    "view.addModel(open('md_n.xyz').read(), format='xyz')\n",
    "for i,type in enumerate(atype[0]):\n",
    "    view.setStyle({'index':i}, {'sphere': {'scale':setting[type][0],'color':'#'+self_hex(value[i])+self_hex(value[i])+self_hex(value[i])}})\n",
    "view.zoomTo(animate=True)\n",
    "view.show()\n",
    "\n",
    "# 分子图\n",
    "view_2 = py3Dmol.view(width=300, height=300)\n",
    "view_2.addModel(open('md_n.xyz').read(), format='xyz')\n",
    "view.setStyle({'sphere': {'scale': 0.3}})\n",
    "view.zoomTo(animate=True)\n",
    "view.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
